{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Architecture of the learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to design a simple algorithm to distinguish cat images from non-cat images.\n",
    "\n",
    "You will build a Logistic Regression, using a Neural Network mindset. The following Figure explains why **Logistic Regression is actually a very simple Neural Network!**\n",
    "\n",
    "<img src=\"LogReg_kiank.png\" style=\"width:650px;height:400px;\">\n",
    "\n",
    "**Mathematical expression of the algorithm**:\n",
    "\n",
    "For one example $x^{(i)}$:\n",
    "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n",
    "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n",
    "\n",
    "The cost is then computed by summing over all training examples:\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}$$\n",
    "\n",
    "**Key steps**:\n",
    "In this exercise, you will carry out the following steps: \n",
    "    - Initialize the parameters of the model\n",
    "    - Learn the parameters for the model by minimizing the cost  \n",
    "    - Use the learned parameters to make predictions (on the test set)\n",
    "    - Analyse the results and conclude\n",
    "    \n",
    "\n",
    "\n",
    "<b>The main steps for building a Neural Network are: </b>\n",
    "1. Define the model structure (such as number of input features) \n",
    "2. Initialize the model's parameters\n",
    "3. Loop:\n",
    "    - Calculate current loss (forward propagation)\n",
    "    - Calculate current gradient (backward propagation)\n",
    "    - Update parameters (gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Logistics regression implementation <h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Booston_housing_price_df = pd.read_csv('Breast_cancer.csv')\n",
    "\n",
    "X = Booston_housing_price_df.drop([\"malignant_benign\"],axis=1)\n",
    "y = Booston_housing_price_df[\"malignant_benign\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Initialize weight and bias with zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_weight_bias(dim):\n",
    "    w = np.zeros((1,dim))*.001\n",
    "    bias=0\n",
    "    return w,bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Forward propogation and backward propogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<b>Forward Propagation:</b>\n",
    "- You get X\n",
    "- You compute $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "- You calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n",
    "\n",
    "\n",
    "<b>Backward Propagation: </b>\n",
    "\n",
    "Here are the two formulas you will be using: \n",
    "\n",
    "$$ dw =\\frac{\\partial J}{\\partial w} = \\frac{1}{m}(A-Y)X^T\\tag{7}$$\n",
    "$$ db = \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<b> Regularization: </b>\n",
    "\n",
    "  Now that we have an understanding of how regularization helps in reducing overfitting, we’ll learn a few different techniques in order to apply regularization in deep learning.\n",
    "\n",
    " \n",
    "L1(i.e Lasso Regression ) and L2(i.e Ridge Regression )are the most common types of regularization. These update the general cost function by adding another term known as the regularization term.\n",
    "\n",
    "                    Cost function = Loss (say, binary cross entropy) + Regularization term\n",
    "\n",
    "Due to the addition of this regularization term, the values of weight matrices decrease because it assumes that a neural network with smaller weight matrices leads to simpler models. Therefore, it will also reduce overfitting to quite an extent.\n",
    "\n",
    "However, this regularization term differs in L1 and L2.\n",
    "\n",
    "<b>Ridge Regression (L2):</b> <br>\n",
    "Performs L2 regularization, i.e. adds penalty equivalent to square of the magnitude of coefficients\n",
    "Minimization objective = LS Obj + α * (sum of square of coefficients)<br>\n",
    "<b>Lasso Regression (L1):</b><br>\n",
    "Performs L1 regularization, i.e. adds penalty equivalent to absolute value of the magnitude of coefficients\n",
    "Minimization objective = LS Obj + α * (sum of absolute value of coefficients)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1  Forward propogation and backward propogation without regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propogate_without_regularization(w,b,X,Y):\n",
    "\n",
    "    #----Length of input file\n",
    "\n",
    "    N= X.shape[1]\n",
    "\n",
    "    #--- forward propogation\n",
    "\n",
    "    z = np.dot(w, X) +b\n",
    "\n",
    "    A= 1/(1+np.exp(-z))     \n",
    "\n",
    "    #--- backward propagtion\n",
    "\n",
    "    dw = (1/N) * np.dot((A-Y),X.T)\n",
    "\n",
    "    db = (1/N) * np.sum((A-Y))\n",
    "\n",
    "    grads={ \"dw\":dw,\n",
    "\n",
    "            \"db\":db }\n",
    "\n",
    " \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Forward propogation and backward propogation with Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propogate_with_regularization(w,b,X,Y,alpha,regularization):\n",
    "\n",
    "    #----Length of input file\n",
    "\n",
    "    N= X.shape[1]\n",
    "\n",
    "    #--- forward propogation\n",
    "\n",
    "    z = np.dot(w, X) +b\n",
    "\n",
    "    A= 1/(1+np.exp(-z)) \n",
    "\n",
    " \n",
    "    #--- backward propagtion\n",
    "\n",
    "    dw = (1/N) * ( np.dot((A-Y),X.T) + alpha * w )\n",
    "\n",
    "    db = (1/N) * np.sum((A-Y))\n",
    "\n",
    "   \n",
    "\n",
    "    grads={ \"dw\":dw,\n",
    "\n",
    "            \"db\":db }\n",
    "\n",
    "  \n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w,b):\n",
    "\n",
    "    #----Length of input file\n",
    "\n",
    "    N= X.shape[1]\n",
    "    Y_prediction = np.zeros((1,N),dtype=np.int)\n",
    "\n",
    "    z = np.dot(w, X) +b\n",
    "\n",
    "    predictions= 1/(1+np.exp(-z))\n",
    "\n",
    "    for i in range(N):\n",
    "\n",
    "        if  predictions[0,i] >.5:\n",
    "\n",
    "            \n",
    "\n",
    "            Y_prediction[0,i] = 1\n",
    "\n",
    "        else:\n",
    "\n",
    "            Y_prediction[0,i] = 0\n",
    "\n",
    "        \n",
    "\n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 : Optimization without regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_without_regularization(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "\n",
    "    N= X.shape[1]\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        grads = propogate_without_regularization(w,b,X,Y)\n",
    "\n",
    "         # Retrieve derivatives from grads\n",
    "\n",
    "        dw = grads[\"dw\"]\n",
    "\n",
    "        db = grads[\"db\"]\n",
    "\n",
    "  \n",
    "        w = w - learning_rate * dw\n",
    "\n",
    "        b = b - learning_rate * db\n",
    "\n",
    "        z = np.dot(w, X) +b\n",
    "\n",
    "        A= 1/(1+np.exp(-z)) \n",
    "\n",
    " \n",
    "        cross_entropy_cost = -(1/N) * np.sum((Y*np.log(A) +(1-Y)*np.log(1-A)))\n",
    "\n",
    "        if print_cost and i%100 == 0:\n",
    "\n",
    "            print (\"iter={:d}   cost={:f}\".format(i, cross_entropy_cost))\n",
    "\n",
    " \n",
    "    return w,b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 : Optimization with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_with_regularization(w, b, X, Y, num_iterations, learning_rate,alpha,regularization, print_cost = False):\n",
    "\n",
    "    N= X.shape[1]\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        grads = propogate_with_regularization (w,b,X,Y,alpha,regularization)\n",
    "\n",
    "  \n",
    "         # Retrieve derivatives from grads\n",
    "\n",
    "        dw = grads[\"dw\"]\n",
    "\n",
    "        db = grads[\"db\"]\n",
    "\n",
    " \n",
    "        w = w - learning_rate * dw\n",
    "\n",
    "        b = b - learning_rate * db\n",
    "\n",
    "   \n",
    "        z = np.dot(w, X) +b\n",
    "\n",
    "        A= 1/(1+np.exp(-z)) \n",
    "\n",
    "        cross_entropy_cost = -(1/N) * np.sum((Y*np.log(A) +(1-Y)*np.log(1-A)))\n",
    "\n",
    "        L2_regularization_cost = np.sum(np.square(w))/(2*N)\n",
    "\n",
    "        cost = cross_entropy_cost + L2_regularization_cost\n",
    "\n",
    "  \n",
    "        if print_cost and i%10 == 0:\n",
    "\n",
    "            print (\"iter={:d}   cost={:f}\".format(i, cost))\n",
    "\n",
    "            \n",
    "\n",
    "    return w,b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 : Logistics regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 20, learning_rate = 0.5, alpha=0, regularization=\"\", print_cost = False):\n",
    "\n",
    "    w, b = initialization_weight_bias(X_train.shape[0])\n",
    "\n",
    " \n",
    "\n",
    "    # Gradient descent (≈ 1 line of code)\n",
    "\n",
    "    if regularization == \"L2\":\n",
    "\n",
    "        w ,b = optimize_with_regularization(w, b, X_train, Y_train, num_iterations, learning_rate,alpha, \n",
    "\n",
    "                                            regularization, print_cost)\n",
    "\n",
    "     \n",
    "\n",
    "    else:\n",
    "\n",
    "        w ,b = optimize_without_regularization(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "\n",
    "        \n",
    "\n",
    "   \n",
    "\n",
    "    print(w)\n",
    "\n",
    "    print(b)\n",
    "\n",
    "    # Predict test/train set examples (≈ 2 lines of code)\n",
    "\n",
    "    Y_prediction_test  = predict(X_test,w, b, )\n",
    "\n",
    "    Y_prediction_train = predict(X_train,w, b)\n",
    "\n",
    " \n",
    "\n",
    "   \n",
    "\n",
    "    # Print train/test Errors\n",
    "\n",
    "    print(\"train cross entropy cost: {} \".format(np.sqrt(np.mean(np.square(Y_prediction_train - Y_train)))))\n",
    "\n",
    "    print(\"test  cross entropy cost: {} \".format(np.sqrt(np.mean(np.square(Y_prediction_test - Y_test)))))\n",
    "\n",
    "    d = {\"Y_prediction_test\": Y_prediction_test, \n",
    "\n",
    "         \"Y_prediction_train\" : Y_prediction_train}\n",
    "\n",
    "    \n",
    "\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7: Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=0   cost=4.019401\n",
      "iter=100   cost=inf\n",
      "iter=200   cost=0.846546\n",
      "iter=300   cost=0.881552\n",
      "[[ 2.62453814e-02  4.49796073e-02  1.57780611e-01  1.13174703e-01\n",
      "   2.66201517e-04  2.82383346e-05 -2.59760622e-04 -1.22292064e-04\n",
      "   5.02951898e-04  2.05329709e-04  1.38832835e-04  3.38301547e-03\n",
      "   3.70426510e-04 -6.03123644e-02  1.98997207e-05  1.97940429e-05\n",
      "   1.16560968e-05  9.78538404e-06  5.61438228e-05  8.84572518e-06\n",
      "   2.66739676e-02  5.75536412e-02  1.58307458e-01 -1.30651710e-01\n",
      "   3.47213334e-04 -5.78467119e-05 -4.38762857e-04 -9.15086837e-05\n",
      "   7.13489690e-04  2.21703473e-04]]\n",
      "0.003386123509735405\n",
      "train cross entropy cost: 0.3144854510165755 \n",
      "test  cross entropy cost: 0.24779731389167603 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "M:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:25: RuntimeWarning: divide by zero encountered in log\n",
      "M:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    }
   ],
   "source": [
    "X_train_val = X_train.values.T\n",
    "X_test_val  = X_test.values.T\n",
    "y_train_val = y_train.values.T\n",
    "y_test_val  = y_test.values.T\n",
    "\n",
    " \n",
    "\n",
    "d = model(X_train_val, y_train_val, X_test_val, y_test_val, num_iterations = 400, learning_rate = .0001, \n",
    "\n",
    "          alpha =.1,regularization=\"\",\n",
    "\n",
    "          print_cost = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General steps of Linera regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<b>The main steps for Linera Regression are: </b>\n",
    "1. Define the model structure (such as number of input features) \n",
    "2. Initialize the model's parameters\n",
    "3. Loop:\n",
    "    - Calculate current cost (forward propagation)\n",
    "    - Calculate current gradient (backward propagation)\n",
    "    - Update parameters (gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Booston_housing_price_df = pd.read_csv('Booston_housing_price_prediction.csv')\n",
    "\n",
    "X = Booston_housing_price_df.drop([\"MEDV\"],axis=1)\n",
    "y = Booston_housing_price_df[\"MEDV\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Initialize weight and bias with zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_weight_bias(dim):\n",
    "    w = np.zeros((1,dim))*.001\n",
    "    bias=0\n",
    "    return w,bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Forward propogation and backward propogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<b>Forward Propagation:</b>\n",
    "- You get X\n",
    "- You compute $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "- You calculate the cost function: $J = -\\frac{1}{2m}\\sum_{i=1}^{m}(y^{(i)}-a^{(i)})^2$\n",
    "\n",
    "\n",
    " \n",
    " \n",
    " \n",
    "<b>Backward Propagation: </b>\n",
    "\n",
    "Here are the two formulas you will be using: \n",
    "\n",
    "$$ dw =\\frac{\\partial J}{\\partial w} = \\frac{1}{m}(A-Y)X^T\\tag{7}$$\n",
    "$$ db = \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<b> Regularization: </b>\n",
    "\n",
    "  Now that we have an understanding of how regularization helps in reducing overfitting, we’ll learn a few different techniques in order to apply regularization in deep learning.\n",
    "\n",
    " \n",
    "L1(i.e Lasso Regression ) and L2(i.e Ridge Regression )are the most common types of regularization. These update the general cost function by adding another term known as the regularization term.\n",
    "\n",
    "                    Cost function = Loss (say, binary cross entropy) + Regularization term\n",
    "\n",
    "Due to the addition of this regularization term, the values of weight matrices decrease because it assumes that a neural network with smaller weight matrices leads to simpler models. Therefore, it will also reduce overfitting to quite an extent.\n",
    "\n",
    "However, this regularization term differs in L1 and L2.\n",
    "\n",
    "<b>Ridge Regression (L2):</b> <br>\n",
    "Performs L2 regularization, i.e. adds penalty equivalent to square of the magnitude of coefficients\n",
    "Minimization objective = LS Obj + α * (sum of square of coefficients)<br>\n",
    "<b>Lasso Regression (L1):</b><br>\n",
    "Performs L1 regularization, i.e. adds penalty equivalent to absolute value of the magnitude of coefficients\n",
    "Minimization objective = LS Obj + α * (sum of absolute value of coefficients)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1  Forward propogation and backward propogation without regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propogate_without_regularization(w,b,X,Y):\n",
    "   #----Length of input file\n",
    "\n",
    "    N= X.shape[1]\n",
    "\n",
    "    #--- forward propogation\n",
    "\n",
    "    A = np.dot(w,X)+b\n",
    "\n",
    "    error = Y-A\n",
    "\n",
    "    #--- backward propagtion\n",
    "\n",
    "    dw = -(1/N) * np.dot(error,X.T)\n",
    "\n",
    "    db = -(1/N) * np.sum(error)\n",
    "\n",
    "    \n",
    "\n",
    "    grads={ \"dw\":dw,\n",
    "\n",
    "            \"db\":db }\n",
    "  \n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Forward propogation and backward propogation with Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propogate_with_regularization(w,b,X,Y,alpha,regularization):\n",
    "\n",
    "    N= X.shape[1]\n",
    "\n",
    "    #--- forward propogation\n",
    "\n",
    "    A = np.dot(w,X)+b\n",
    "\n",
    "    error = Y-A\n",
    "    #--- backward propagtion\n",
    "\n",
    "   \n",
    "\n",
    "    dw = -(1/N) * ( np.dot(error,X.T) + alpha*w)\n",
    "\n",
    "    db = -(1/N) * np.sum(error)\n",
    "\n",
    "    grads={ \"dw\":dw,\n",
    "\n",
    "            \"db\":db }\n",
    "\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w,b):\n",
    "    predictions = np.dot(w, X) +b\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 : Optimization without regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_without_regularization(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    N= X.shape[1]\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        grads = propogate_without_regularization(w,b,X,Y)\n",
    "\n",
    "         # Retrieve derivatives from grads\n",
    "\n",
    "        dw = grads[\"dw\"]\n",
    "\n",
    "        db = grads[\"db\"]\n",
    "\n",
    "        w = w - learning_rate * dw\n",
    "\n",
    "        b = b - learning_rate * db\n",
    "\n",
    "        predictions = predict(X, w, b)\n",
    "\n",
    "        cross_entropy_cost = np.sum(np.square(Y - predictions))/(2*N)\n",
    "\n",
    "        cost = cross_entropy_cost\n",
    "\n",
    "\n",
    "        if print_cost and i%10 == 0:\n",
    "\n",
    "            print (\"iter={:d}   cost={:.2}\".format(i, cost))\n",
    "\n",
    "            \n",
    "    return w,b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 : Optimization with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_with_regularization(w, b, X, Y, num_iterations, learning_rate,alpha,regularization, print_cost = False):\n",
    "    N= X.shape[1]\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        grads = propogate_with_regularization (w,b,X,Y,alpha,regularization)\n",
    "\n",
    "\n",
    "         # Retrieve derivatives from grads\n",
    "\n",
    "        dw = grads[\"dw\"]\n",
    "\n",
    "        db = grads[\"db\"]\n",
    "\n",
    "\n",
    "        w = w - learning_rate * dw\n",
    "\n",
    "        b = b - learning_rate * db\n",
    "\n",
    "  \n",
    "        predictions = predict(X, w, b)\n",
    "\n",
    "        cross_entropy_cost = np.sum(np.square(Y - predictions))/(2*N)\n",
    "\n",
    "        L2_regularization_cost = np.sum(np.square(w))/(2*N)\n",
    "\n",
    "        cost = cross_entropy_cost + L2_regularization_cost\n",
    "\n",
    "   \n",
    "        if print_cost and i%10 == 0:\n",
    "\n",
    "            print (\"iter={:d}   cost={:.2}\".format(i, cost))\n",
    "\n",
    "  \n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 : Linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 20, learning_rate = 0.5, alpha=0, regularization=\"\", print_cost = False):\n",
    "    \n",
    "    w, b = initialization_weight_bias(X_train.shape[0])\n",
    "    \n",
    "    # Gradient descent (≈ 1 line of code)\n",
    "\n",
    "    if regularization == \"L2\":\n",
    "\n",
    "        w ,b = optimize_with_regularization(w, b, X_train, Y_train, num_iterations, learning_rate,alpha, \n",
    "\n",
    "                                            regularization, print_cost)\n",
    "\n",
    "     \n",
    "\n",
    "    else:\n",
    "\n",
    "        w ,b = optimize_without_regularization(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "\n",
    "  \n",
    "    print(w)\n",
    "\n",
    "    print(b)\n",
    "\n",
    "    # Predict test/train set examples (≈ 2 lines of code)\n",
    "\n",
    "    Y_prediction_test  = predict(X_test,w, b, )\n",
    "\n",
    "    Y_prediction_train = predict(X_train,w, b)\n",
    "\n",
    "    # Print train/test Errors\n",
    "\n",
    "    print(\"train RMSE: {} \".format(np.sqrt(np.mean(np.square(Y_prediction_train - Y_train)))))\n",
    "\n",
    "    print(\"test  RMSE: {} \".format(np.sqrt(np.mean(np.square(Y_prediction_test - Y_test)))))\n",
    "\n",
    " \n",
    "    d = {\"Y_prediction_test\": Y_prediction_test, \n",
    "\n",
    "         \"Y_prediction_train\" : Y_prediction_train}\n",
    "\n",
    "    \n",
    "\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7: Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=0   cost=1.2e+03\n",
      "iter=10   cost=4.9e+09\n",
      "iter=20   cost=2.2e+16\n",
      "iter=30   cost=9.9e+22\n",
      "iter=40   cost=4.4e+29\n",
      "iter=50   cost=2e+36\n",
      "iter=60   cost=8.8e+42\n",
      "iter=70   cost=4e+49\n",
      "iter=80   cost=1.8e+56\n",
      "iter=90   cost=7.9e+62\n",
      "iter=100   cost=3.6e+69\n",
      "iter=110   cost=1.6e+76\n",
      "iter=120   cost=7.1e+82\n",
      "iter=130   cost=3.2e+89\n",
      "iter=140   cost=1.4e+96\n",
      "iter=150   cost=6.4e+102\n",
      "iter=160   cost=2.9e+109\n",
      "iter=170   cost=1.3e+116\n",
      "iter=180   cost=5.8e+122\n",
      "iter=190   cost=2.6e+129\n",
      "iter=200   cost=1.2e+136\n",
      "[[-1.94145047e+66 -4.81324803e+66 -5.65462641e+66 -3.68590721e+64\n",
      "  -2.65509505e+65 -2.91764745e+66 -3.31911295e+67 -1.64793732e+66\n",
      "  -5.09945627e+66 -2.04944492e+68 -8.57533844e+66 -1.66523988e+68\n",
      "  -6.11015347e+66]]\n",
      "-4.642257691368154e+65\n",
      "train RMSE: 1.4960687199328814e+71 \n",
      "test  RMSE: 1.4813005798811312e+71 \n"
     ]
    }
   ],
   "source": [
    "X_train_val = X_train.values.T\n",
    "X_test_val  = X_test.values.T\n",
    "y_train_val = y_train.values.T\n",
    "y_test_val  = y_test.values.T\n",
    "\n",
    "\n",
    "d = model(X_train_val, y_train_val, X_test_val, y_test_val, num_iterations = 210, learning_rate = .00001, \n",
    "          alpha =.1,regularization=\" \",print_cost = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
